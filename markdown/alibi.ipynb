{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d83cb9-f38a-4457-be75-d5878e5e740d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "69aa1c42-86d9-41e3-bd66-b6d9d07e0fa0",
   "metadata": {},
   "source": [
    "ALiBi（Attention with Linear Biases）是一种针对Transformer模型的注意力机制改进，旨在解决现有位置编码方法在处理超出训练序列长度的输入时效率低下或资源需求高的问题。ALiBi通过在注意力机制中引入静态、非学习的线性偏差来增强模型对长序列的处理能力，而无需增加额外的计算负担或改变模型结构。\n",
    "\n",
    "具体来说，ALiBi通过在注意力分数计算中加入与查询（q）和键（k）之间距离成正比的线性偏差，来模拟人类阅读时对前文内容记忆逐渐减弱的自然趋势。这种方法使得模型能够更有效地处理长序列输入，同时保持了对远处上下文的敏感性。此外，由于ALiBi使用的是固定的线性偏差，它避免了在模型中增加过多的参数，从而节省了内存和计算资源。\n",
    "\n",
    "ALiBi的另一个优势是其泛化能力，它的线性偏差设置具有普适性，因此无需针对新的数据集或文本域调整超参数，这与需要为每种新的序列长度或数据集调整或重新学习位置编码的原有位置编码方法形成对比。\n",
    "\n",
    "此外，FlashAttention是一个与ALiBi相辅相成的技术，它是一种新型的注意力机制，用于提高计算速度和内存使用效率，同时保持精确的结果。FlashAttention通过一系列创新技术优化，显著提高了注意力机制的计算速度，并减少了内存占用，特别是在处理长序列数据时。尽管FlashAttention本身并不直接依赖于ALiBi，但它们都致力于解决Transformer模型在处理长序列数据时面临的效率问题。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
