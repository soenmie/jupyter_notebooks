{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b4bdca5-e08f-4973-8973-a810e8ea2a53",
   "metadata": {},
   "source": [
    "# batchnorm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7c48f0-a19f-4495-902e-7df89aab1953",
   "metadata": {},
   "source": [
    "批量归一化（Batch Normalization, 简称 Batch Norm 或 BN）是深度学习中常用的一种技术，它用以减少内部协变量偏移（Internal Covariate Shift），加速模型学习过程，同时还能起到一定的正则化效果。\n",
    "\n",
    "Batch Norm 在训练（Training）和预测（Inference）的时候，处理统计值的方式是不同的。下面分别描述两个阶段的处理方式：\n",
    "\n",
    "## 训练过程\n",
    "\n",
    "在训练阶段，Batch Norm 每次处理一个 mini-batch 的数据。它首先计算当前 mini-batch 的均值（mean）和方差（variance），然后使用这些统计值来归一化当前的 mini-batch 数据。\n",
    "\n",
    "设有输入数据 $X = \\{x_1, x_2, ..., x_m\\}$，其中 $m$ 是当前 mini-batch 的大小，Batch Norm 计算均值 ($\\mu_B$) 和方差 ($\\sigma_B^2$) 如下：\n",
    "\n",
    "$$\n",
    "\\mu_B = \\frac{1}{m} \\sum_{i=1}^{m} x_i\n",
    "$$\n",
    "$$\n",
    "\\sigma_B^2 = \\frac{1}{m} \\sum_{i=1}^{m} (x_i - \\mu_B)^2\n",
    "$$\n",
    "\n",
    "然后使用这两个值对输入 $X$ 进行归一化：\n",
    "\n",
    "$$\n",
    "\\hat{X} = \\frac{X - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}}\n",
    "$$\n",
    "\n",
    "其中的 $\\epsilon$ 是一个很小的数（例如 1e-5），以确保分母不为零，是为了数值稳定性考虑。\n",
    "\n",
    "归一化之后，Batch Norm 还提供了两个可学习参数，缩放因子（γ, scale）和位移（β, shift），用以恢复到可能的最佳的分布状态：\n",
    "\n",
    "$$\n",
    "Y = \\gamma \\hat{X} + \\beta\n",
    "$$\n",
    "\n",
    "这里的 $Y$ 就是 Batch Norm 层的输出。在训练阶段，γ 和 β 是通过反向传播与其他模型参数一同训练的。\n",
    "\n",
    "## 预测过程（Inference）\n",
    "\n",
    "在预测（或测试）阶段，模型会处理可能非常不同的数据，并且通常是一个一个样本地处理，而不是 mini-batch。如果继续使用 mini-batch 的统计值去归一化，那么会因为每次的 mini-batch 不同导致归一化的不稳定，甚至不可能计算（当只有一个数据点时）。\n",
    "\n",
    "因此，在预测时，Batch Norm 使用在训练过程中累积（通常是指数加权移动平均）的均值和方差的估计：\n",
    "\n",
    "$$\n",
    "\\mu = E[\\mu_B]\n",
    "$$\n",
    "$$\n",
    "\\sigma^2 = E[\\sigma_B^2]\n",
    "$$\n",
    "\n",
    "这里的 $E[\\cdot]$ 表示估计值。这些值是在训练过程中计算并更新的，不再随 mini-batch 的不同而变化。\n",
    "\n",
    "预测时的 Batch Norm 归一化过程如下：\n",
    "\n",
    "$$\n",
    "\\hat{x}_{\\text{test}} = \\frac{x_{\\text{test}} - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}}\n",
    "$$\n",
    "\n",
    "对应的输出是：\n",
    "\n",
    "$$\n",
    "y_{\\text{test}} = \\gamma \\hat{x}_{\\text{test}} + \\beta\n",
    "$$\n",
    "\n",
    "其中 $x_{\\text{test}}$ 是单个测试样本，$y_{\\text{test}}$ 是该样本通过 Batch Norm 层的输出。\n",
    "\n",
    "通过以上的操作，Batch Norm 使得模型在预测时能够使用在训练过程中学到的、更为平稳的数据分布，提高泛化能力。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f87dcf1d-f549-4080-b0a7-72a23d444c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# With Learnable Parameters\n",
    "m = nn.BatchNorm1d(100)\n",
    "# Without Learnable Parameters\n",
    "# m = nn.BatchNorm1d(100, affine=False)\n",
    "input = torch.randn(20, 100)\n",
    "output = m(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a9b19a6-b630-4939-89e2-9698f1f980ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: weight,\tShape: torch.Size([100]),\tRequires grad: True\n",
      "Name: bias,\tShape: torch.Size([100]),\tRequires grad: True\n"
     ]
    }
   ],
   "source": [
    "# 打印模型的所有具名参数\n",
    "for name, param in m.named_parameters():\n",
    "    print(f\"Name: {name},\\tShape: {param.size()},\\tRequires grad: {param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0bfe2e92-32c3-48af-ae6d-caa6fe3d3429",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4badf839-a1db-41d7-ac2b-c805214c8355",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 2.2526e-02,  8.3788e-03,  2.5479e-02,  2.0515e-02, -5.3198e-02,\n",
       "          1.0445e-02, -8.7572e-03,  1.1553e-02,  4.9583e-03, -2.7973e-02,\n",
       "         -3.1462e-02,  2.2348e-02,  3.2692e-02, -2.1632e-02, -4.3830e-02,\n",
       "         -2.5293e-02,  6.9338e-03,  2.0425e-02,  2.6334e-03,  2.4866e-02,\n",
       "         -9.4535e-03, -1.7107e-03,  2.1068e-02, -2.2040e-02, -3.5879e-02,\n",
       "          6.4485e-04, -8.6509e-03, -1.4788e-02,  3.4774e-02,  5.8979e-03,\n",
       "          6.8799e-03, -2.2506e-02,  2.3639e-02, -1.1672e-02, -2.2402e-02,\n",
       "          2.3265e-02, -4.0368e-02, -9.8057e-03,  3.3580e-03, -1.2860e-02,\n",
       "          5.7512e-03, -2.2749e-02, -7.0760e-03, -1.2083e-02, -1.8619e-02,\n",
       "         -2.8901e-02, -1.4613e-02,  8.0565e-03,  1.4769e-02,  6.6605e-03,\n",
       "         -1.2035e-02, -4.4077e-03,  2.9764e-02, -1.9618e-02, -3.1000e-02,\n",
       "          1.8648e-02, -9.0123e-03,  4.5604e-03,  3.1216e-02,  1.4658e-02,\n",
       "         -1.0180e-02,  4.4239e-03,  3.5542e-02,  1.7736e-02, -1.7775e-02,\n",
       "          2.0883e-02,  7.7922e-03, -7.0722e-03, -2.7812e-02,  2.4205e-02,\n",
       "          1.8099e-02,  1.1498e-02, -8.5181e-05,  6.5165e-02, -4.7854e-02,\n",
       "          1.7279e-02,  6.4373e-03,  1.5620e-02, -1.4807e-03, -1.8502e-02,\n",
       "         -1.8526e-02, -3.9151e-02, -4.5826e-03,  6.1504e-03,  4.0938e-02,\n",
       "          8.3081e-03,  3.9017e-03,  1.2230e-02,  3.0189e-02, -5.1400e-03,\n",
       "          3.0777e-02, -2.2377e-02,  1.5115e-02,  2.5223e-02,  3.6710e-02,\n",
       "         -2.0786e-02, -2.8701e-03, -1.8453e-02, -3.5029e-03, -9.1527e-03]),\n",
       " tensor([0.9693, 1.0120, 0.9899, 1.0079, 0.9574, 1.0657, 0.9968, 0.9916, 0.9639,\n",
       "         1.0435, 1.0854, 1.0279, 1.0489, 0.9960, 0.9815, 0.9861, 0.9683, 1.0217,\n",
       "         1.0431, 0.9821, 1.0467, 1.0330, 0.9876, 0.9496, 0.9639, 0.9943, 1.0305,\n",
       "         0.9594, 0.9709, 1.0124, 0.9428, 0.9895, 1.0504, 1.0052, 0.9549, 0.9934,\n",
       "         0.9904, 1.0114, 1.0389, 0.9666, 0.9847, 1.0102, 1.0524, 1.0000, 0.9840,\n",
       "         0.9966, 0.9976, 1.0142, 0.9735, 0.9871, 0.9766, 1.0162, 1.0248, 0.9778,\n",
       "         1.0318, 1.0443, 1.0202, 0.9779, 0.9828, 0.9819, 0.9970, 0.9929, 1.0492,\n",
       "         1.0010, 0.9750, 1.0416, 1.0326, 0.9751, 0.9949, 0.9980, 1.0213, 0.9900,\n",
       "         1.0098, 0.9761, 1.0007, 0.9646, 0.9828, 0.9721, 0.9738, 1.0364, 0.9483,\n",
       "         1.0140, 1.0481, 0.9816, 1.0503, 1.0251, 1.0157, 0.9711, 1.0626, 0.9895,\n",
       "         1.0636, 1.0621, 1.0492, 0.9938, 1.0053, 1.0406, 0.9866, 1.0495, 1.0043,\n",
       "         0.9509]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.running_mean, m.running_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eeafc8e6-1fdc-4e42-bcde-3633604a7236",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1), True)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.num_batches_tracked, m.training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258b6a7f-4882-4795-b512-dd86bfdf5d6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
