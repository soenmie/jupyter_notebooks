{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b4b0e9-9278-4cc4-95b6-d9939c4c2eea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "118c0ae1-02ec-486f-96b2-3ef6097e31bd",
   "metadata": {},
   "source": [
    "在构建一个大型语言模型的transformer layer时，我们可以使用PyTorch这样的深度学习框架，因为PyTorch内置了多头注意力层 (`MultiheadAttention`)，使得我们可以很容易地搭建并训练模型。以下是一个基本的Transformer编码器层的实现，它包含一个多头注意力层和一个前馈网络，加上残差连接和层归一化。\n",
    "\n",
    "我们先引入必要的库和函数：\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TransformerLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dim_feedforward, dropout=0.1):\n",
    "        super(TransformerLayer, self).__init__()\n",
    "        \n",
    "        # 多头注意力层\n",
    "        self.multihead_attention = nn.MultiheadAttention(d_model, num_heads, dropout=dropout)\n",
    "\n",
    "        # 用于注意力层的残差连接后的归一化层\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "\n",
    "        # 用于前馈网络的残差连接后的归一化层\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "        # 前馈网络（两层全连接层）\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(d_model, dim_feedforward),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim_feedforward, d_model)\n",
    "        )\n",
    "\n",
    "        # 层之间的dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src, src_mask=None, src_key_padding_mask=None):\n",
    "        # Self-attention\n",
    "        attn_output, _ = self.multihead_attention(src, src, src, attn_mask=src_mask, key_padding_mask=src_key_padding_mask)\n",
    "        \n",
    "        # 经过残差连接然后是LayerNorm\n",
    "        src = src + self.dropout(attn_output)\n",
    "        src = self.norm1(src)\n",
    "        \n",
    "        # 前馈网络\n",
    "        ff_output = self.feed_forward(src)\n",
    "        \n",
    "        # 残差连接和LayerNorm\n",
    "        src = src + self.dropout(ff_output)\n",
    "        src = self.norm2(src)\n",
    "        \n",
    "        return src\n",
    "```\n",
    "在上面的代码中，`TransformerLayer`类定义了一个标准的Transformer编码器层。它接受以下参数：\n",
    "\n",
    "- `d_model`: 输入数据的特征维度，同时也是模型的维度。\n",
    "- `num_heads`: 多头注意力层中的头数量。\n",
    "- `dim_feedforward`: 前馈网络中的内部层的维度。\n",
    "- `dropout`: 避免过拟合的dropout比率。\n",
    "\n",
    "`forward`方法接受以下输入：\n",
    "\n",
    "- `src`: 输入序列的张量，它的形状通常是 `(seq_length, batch_size, d_model)`。\n",
    "- `src_mask`: 可选参数，用于掩盖（masking）序列中的某些位置，通常是出于padding或者future information的考虑。\n",
    "- `src_key_padding_mask`: 可选参数，用于在batch中的各个序列可能有不同长度时对padding位置进行掩盖。\n",
    "\n",
    "该实现遵循了Attention is All You Need论文中描述的结构，并添加了适量的dropout以减少过拟合的风险。这个层可以嵌入到一个更大的模型中，比如完整的Transformer模型，由多个这样的层堆叠在一起构成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885bda7d-bbbc-4268-8173-66a75d408761",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e1905b-f7f7-4614-8ce9-2d82e77a2b8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d933bf-5aad-455e-8211-4a296f4978ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://pytorch.org/docs/stable/_modules/torch/nn/modules/activation.html#MultiheadAttention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f66d20-487a-44fa-8a83-f08d28e9d0c3",
   "metadata": {},
   "source": [
    "在PyTorch中实现一个简单的注意力层可以通过定义一个继承自`torch.nn.Module`的类来完成。在这个实现中，我们将创建一个基于点积（scaled dot-product）注意力机制的层。这个注意力机制通常在许多变体如Transformer模型中看到。\n",
    "\n",
    "下面是一个简单的实现：\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        \n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_size // heads\n",
    "        \n",
    "        assert (\n",
    "            self.head_dim * heads == embed_size\n",
    "        ), \"Embed size needs to be divisible by heads\"\n",
    "        \n",
    "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n",
    "\n",
    "    def forward(self, values, keys, query, mask):\n",
    "        N = query.shape[0]\n",
    "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
    "        \n",
    "        # Split the embedding into self.heads different pieces\n",
    "        values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
    "        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
    "        queries = query.reshape(N, query_len, self.heads, self.head_dim)\n",
    "        \n",
    "        values = self.values(values)\n",
    "        keys = self.keys(keys)\n",
    "        queries = self.queries(queries)\n",
    "        \n",
    "        # Scaled dot-product attention\n",
    "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
    "        \n",
    "        # Scale energy by size of the key dimension\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "        \n",
    "        attention = F.softmax(energy / (self.embed_size ** (1 / 2)), dim=3)\n",
    "        \n",
    "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n",
    "            N, query_len, self.heads * self.head_dim\n",
    "        )\n",
    "        \n",
    "        out = self.fc_out(out)\n",
    "        \n",
    "        return out\n",
    "```\n",
    "\n",
    "这个层可以这样使用：\n",
    "\n",
    "```python\n",
    "batch_size = 1\n",
    "query_len = 3\n",
    "key_len = value_len = 3\n",
    "embed_size = 256\n",
    "num_heads = 8\n",
    "\n",
    "# Create random data for query, key, and value\n",
    "query = torch.rand(batch_size, query_len, embed_size)\n",
    "key = torch.rand(batch_size, key_len, embed_size)\n",
    "value = torch.rand(batch_size, value_len, embed_size)\n",
    "\n",
    "# Create a mask for the attention calculation (optional)\n",
    "mask = None  # For example purposes; typically this will be a ByteTensor\n",
    "\n",
    "# Initialize the attention layer\n",
    "attention = AttentionLayer(embed_size, num_heads)\n",
    "\n",
    "# Apply the attention operation\n",
    "output = attention(value, key, query, mask)\n",
    "```\n",
    "\n",
    "注意：在实际使用中，`mask`通常被用于防止模型看到未来的信息或者对填充的token赋予注意力。上面的代码是一个基础版本的注意力层，实际模型中可能需要根据具体用例进行调整。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e746126b-cdad-443f-b55d-86d4b5d85fe1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "55624ba1-c4c6-44c7-bb30-b6d1dd4e1860",
   "metadata": {},
   "source": [
    "在PyTorch中实现一个多头自注意力（Multi-Head Attention）层需要遵循以下步骤：\n",
    "\n",
    "1. 定义一个带有线性层（用于生成查询（query）、键（key）、值（value）的投影）的多头自注意力类。\n",
    "2. 在前向传播中，将输入数据通过这三个线性层以生成查询、键和值的表示。\n",
    "3. 将查询、键、值分割成多个头并进行缩放点积注意力计算。\n",
    "4. 连接多个头的输出，并通过最后一个线性层进行投影得到最终的注意力输出。\n",
    "\n",
    "以下是一个简单的实现示例：\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % num_heads == 0, \"The hidden size (d_model) is not a multiple of the number of heads\"\n",
    "\n",
    "        self.d_model = d_model                  # 输入数据的特征维度\n",
    "        self.num_heads = num_heads              # 注意力头的数量\n",
    "        self.d_k = d_model // num_heads         # 每个头的维度\n",
    "\n",
    "        # 定义线性层\n",
    "        self.query_proj = nn.Linear(d_model, d_model)\n",
    "        self.key_proj = nn.Linear(d_model, d_model)\n",
    "        self.value_proj = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.out_proj = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.size(0)\n",
    "\n",
    "        # 1) 进行线性投影并拆分多头\n",
    "        query = self.query_proj(query).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        key = self.key_proj(key).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        value = self.value_proj(value).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "        # 2) 计算注意力得分，使用缩放点积的形式\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        attention = F.softmax(scores, dim=-1)\n",
    "\n",
    "        # 3) 应用dropout\n",
    "        attention = self.dropout(attention)\n",
    "\n",
    "        # 4) 将获取的值与注意力得分相乘\n",
    "        context = torch.matmul(attention, value)\n",
    "\n",
    "        # 5) 连接多头\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
    "\n",
    "        # 6) 通过最终的线性层\n",
    "        output = self.out_proj(context)\n",
    "\n",
    "        return output\n",
    "```\n",
    "\n",
    "在实际使用时，还需要确定一些超参数，如`d_model`和`num_heads`。以上代码提供了一个基本的模板，您可能需要根据您的具体需求进行调整。`mask`参数可以用于遮蔽序列中的某些元素，这在处理不等长序列或进行解码器自注意力操作时很有用，解码器需要屏蔽未来时刻的信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53576a0-0594-479d-b06b-875672fa468d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
